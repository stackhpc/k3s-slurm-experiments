ClusterName={{ slurm_cluster_name }}
SlurmctldHost={{ slurm_control_address }}
MpiDefault=none
ProctrackType=proctrack/pgid    # from openhpc role, not default
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmctldPort=6817
SlurmdPidFile=/var/run/slurmd.pid
SlurmdPort=6818
SlurmdSpoolDir=/var/spool/slurmd
SlurmUser=slurm
StateSaveLocation=/var/spool/slurmctld
SwitchType=switch/none
TaskPlugin=task/affinity
InactiveLimit=0
KillWait=30
MinJobAge=300
SlurmctldTimeout=120
SlurmdTimeout=300
Waittime=0
TreeWidth=65533
MaxNodeCount=100
#
# SCHEDULING
SchedulerType=sched/backfill
SelectType=select/cons_tres
SelectTypeParameters=CR_Core
#
# JOB PRIORITY

# LOGGING AND ACCOUNTING
AccountingStorageType=accounting_storage/none
JobCompType=jobcomp/none
JobAcctGatherFrequency=30
JobAcctGatherType=jobacct_gather/none
SlurmctldDebug=debug
SlurmctldLogFile=/var/log/slurmctld.log
SlurmdDebug=debug
SlurmdLogFile=/var/log/slurmd.log
#
# OpenHPC default configuration
PropagateResourceLimitsExcept=MEMLOCK
#JobCompType=jobcomp/filetxt # TODO: FIXME:
Epilog=/etc/slurm/slurm.epilog.clean
SlurmctldParameters=enable_configless
ReturnToService=1

# Partition definitions
{% for part_name, part_opts in slurm_partitions.items() %}
NodeSet={{ part_name }} Feature={{ part_name }}
PartitionName={{ part_name }} Nodes={{ part_name }} {{ slurm_partition_defaults | combine(part_opts) | dict2parameters }}
{% endfor %}
